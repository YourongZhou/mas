{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606b603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448012d",
   "metadata": {},
   "source": [
    "## langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a5b172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi~', additional_kwargs={}, response_metadata={}, id='21271a68-5dc8-4d61-913e-68a562d9cfe8'),\n",
       "  AIMessage(content='hello world', additional_kwargs={}, response_metadata={}, id='b4889a49-34cf-43c1-9235-363153be5629', tool_calls=[], invalid_tool_calls=[])]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mock_llm(state: MessagesState):\n",
    "    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(mock_llm)\n",
    "graph.add_edge(START, \"mock_llm\")\n",
    "graph.add_edge(\"mock_llm\", END)\n",
    "graph = graph.compile()\n",
    "\n",
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi~\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f3da45",
   "metadata": {},
   "source": [
    "## æµ‹è¯•qwen (langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a709486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å—¨ï¼æˆ‘æ˜¯ä¸€ååšå£«ç”Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    " \n",
    "llm = ChatOpenAI(\n",
    "    # æ ¹æ®è‡ªå·±çš„éœ€æ±‚é…ç½®ï¼Œå¯ä»¥æ˜¯ç¯å¢ƒå˜é‡ï¼Œä¹Ÿå¯ä»¥æ˜¯æ–‡æœ¬å†…å®¹\n",
    "    api_key=\"sk-3e43aba7e80343bc96fb5e7d549837ac\", \n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\"\n",
    ")\n",
    " \n",
    "messages = [\n",
    "    # ç³»ç»Ÿæç¤ºè¯ï¼šå°†è‹±æ–‡ç¿»è¯‘ä¸ºä¸­æ–‡\n",
    "    SystemMessage(\"Translate the following from English into chinese\"),\n",
    "    HumanMessage(\"hi! I'm a PhD student\"),\n",
    "]\n",
    " \n",
    "aimessages = llm.invoke(messages)\n",
    "print(aimessages.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7a93c",
   "metadata": {},
   "source": [
    "## langgraph + qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fef23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi~ Who are you?', additional_kwargs={}, response_metadata={}, id='25fd6349-dbbc-4b7c-a0be-f8b2c9d6a850'),\n",
       "  AIMessage(content=\"Hello! ğŸ˜Š I'm Qwen, a large-scale language model developed by Alibaba Cloud's Tongyi Lab. I can chat with you, help answer questions, write stories, create documents, code, and even express opinions. I love learning new things and helping out! What would you like to chat about? ğŸŒŸ\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 14, 'total_tokens': 80, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-64f63262-bcc4-9835-9116-cab5261aa827', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bbb67-6e6d-7c32-b7bc-df9fc89e6fb1-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 14, 'output_tokens': 66, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def qwen_llm(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"qwen\", qwen_llm)\n",
    "graph.add_edge(START, \"qwen\")\n",
    "graph.add_edge(\"qwen\", END)\n",
    "graph = graph.compile()\n",
    "\n",
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi~ Who are you?\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843fe1ff",
   "metadata": {},
   "source": [
    "## complex langgraph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3556e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " +-----------+   \n",
      " | __start__ |   \n",
      " +-----------+   \n",
      "        *        \n",
      "        *        \n",
      "        *        \n",
      "+------------+   \n",
      "| math_agent |   \n",
      "+------------+   \n",
      "        *        \n",
      "        *        \n",
      "        *        \n",
      "+-------------+  \n",
      "| check_agent |  \n",
      "+-------------+  \n",
      "        *        \n",
      "        *        \n",
      "        *        \n",
      "  +---------+    \n",
      "  | __end__ |    \n",
      "  +---------+    \n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='1 + 1 = ?', additional_kwargs={}, response_metadata={}, id='b9e34fc9-f8a1-475c-882f-20070865feba'),\n",
       "  AIMessage(content='2', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 46, 'total_tokens': 47, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-b52359e1-aabc-95a8-9275-d02679409f3a', 'finish_reason': 'stop', 'logprobs': None}, name='math_agent', id='lc_run--019bbb67-7860-7771-8920-d51962a22d11-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 46, 'output_tokens': 1, 'total_tokens': 47, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}),\n",
       "  AIMessage(content='correct', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 63, 'total_tokens': 64, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-439e7d46-954d-9861-ba47-9fc8ecd1fae4', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bbb67-7a68-7c51-b900-5d6b4877b3d2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 63, 'output_tokens': 1, 'total_tokens': 64, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç®—æ•° agent\n",
    "def math_agent(state: MessagesState):\n",
    "    system_prompt = SystemMessage(\"\"\"\n",
    "    ä½ æ˜¯ä¸€ä¸ªæ™®é€šçš„è®¡ç®—å‘˜ï¼Œä¼šåšåŸºæœ¬çš„æ•°å­¦è¿ç®—ã€‚\n",
    "    è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚\n",
    "    \"\"\")\n",
    "    messages = [system_prompt] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # æ ‡è®° response åç§°\n",
    "    response.name = \"math_agent\"\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# æ£€æŸ¥ agent\n",
    "def check_agent(state: MessagesState):\n",
    "    system_prompt = SystemMessage(\"\"\"\n",
    "    ä½ æ˜¯ä¸€ä¸ªæ•°å­¦å®¶ï¼Œæ“…é•¿æ£€æŸ¥å„ç§æ•°å­¦é—®é¢˜ã€‚\n",
    "    è¯·æ£€æŸ¥ math_agent çš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚\n",
    "    å¦‚æœæ­£ç¡®ï¼Œè¯·ç»™å‡º \"correct\"ã€‚\n",
    "    å¦‚æœé”™è¯¯ï¼Œè¯·ç»™å‡º \"incorrect\"ã€‚\n",
    "    \"\"\")\n",
    "    messages = [system_prompt] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"math_agent\", math_agent)\n",
    "graph.add_node(\"check_agent\", check_agent)\n",
    "graph.add_edge(START, \"math_agent\")\n",
    "graph.add_edge(\"math_agent\", \"check_agent\")\n",
    "graph.add_edge(\"check_agent\", END)\n",
    "graph = graph.compile()\n",
    "\n",
    "# ç”Ÿæˆå›¾çš„æµç¨‹å›¾ç‰‡ï¼Œå†™åˆ°å½“å‰ç›®å½•ä¸‹\n",
    "qa_async_ascii = graph.get_graph().print_ascii()\n",
    "print(qa_async_ascii)\n",
    "\n",
    "\n",
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"1 + 1 = ?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504ca3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is 3^{12345} (mod 100)?', additional_kwargs={}, response_metadata={}, id='ed16a73e-9415-4b07-a50c-a25324c0b160'),\n",
       "  AIMessage(content='36', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 58, 'total_tokens': 60, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-707b890c-7506-962b-be5a-e8acd4efba89', 'finish_reason': 'stop', 'logprobs': None}, name='math_agent', id='lc_run--019bbb68-5337-7c13-9f97-42fa3ea91128-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 58, 'output_tokens': 2, 'total_tokens': 60, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}),\n",
       "  AIMessage(content=\"We are asked to compute:\\n\\n$$\\n3^{12345} \\\\mod 100\\n$$\\n\\nand check whether the answer **36** is correct.\\n\\n---\\n\\n### Step 1: Use Euler's theorem\\n\\nEulerâ€™s theorem says that if $\\\\gcd(a, m) = 1$, then:\\n\\n$$\\na^{\\\\phi(m)} \\\\equiv 1 \\\\pmod{m}\\n$$\\n\\nHere, $a = 3$, $m = 100$, and $\\\\gcd(3, 100) = 1$, so we can apply it.\\n\\nCompute $\\\\phi(100)$:\\n\\n$$\\n100 = 2^2 \\\\cdot 5^2 \\\\Rightarrow \\\\phi(100) = 100 \\\\left(1 - \\\\frac{1}{2}\\\\right)\\\\left(1 - \\\\frac{1}{5}\\\\right) = 100 \\\\cdot \\\\frac{1}{2} \\\\cdot \\\\frac{4}{5} = 40\\n$$\\n\\nSo,\\n\\n$$\\n3^{40} \\\\equiv 1 \\\\pmod{100}\\n$$\\n\\nNow reduce the exponent modulo 40:\\n\\n$$\\n12345 \\\\mod 40\\n$$\\n\\nDivide:\\n\\n$$\\n12345 \\\\div 40 = 308.625 \\\\Rightarrow 40 \\\\cdot 308 = 12320 \\\\Rightarrow 12345 - 12320 = 25\\n$$\\n\\nSo,\\n\\n$$\\n12345 \\\\equiv 25 \\\\pmod{40}\\n\\\\Rightarrow 3^{12345} \\\\equiv 3^{25} \\\\pmod{100}\\n$$\\n\\nNow compute $3^{25} \\\\mod 100$.\\n\\n---\\n\\n### Step 2: Compute $3^{25} \\\\mod 100$\\n\\nWe can do this via successive squaring.\\n\\nFirst, compute powers of 3 modulo 100:\\n\\n- $3^1 = 3$\\n- $3^2 = 9$\\n- $3^3 = 27$\\n- $3^4 = 81$\\n- $3^5 = 243 \\\\equiv 43 \\\\pmod{100}$\\n- $3^6 = 3 \\\\cdot 43 = 129 \\\\equiv 29 \\\\pmod{100}$\\n- $3^7 = 3 \\\\cdot 29 = 87$\\n- $3^8 = 3 \\\\cdot 87 = 261 \\\\equiv 61$\\n- $3^9 = 3 \\\\cdot 61 = 183 \\\\equiv 83$\\n- $3^{10} = 3 \\\\cdot 83 = 249 \\\\equiv 49$\\n- $3^{11} = 3 \\\\cdot 49 = 147 \\\\equiv 47$\\n- $3^{12} = 3 \\\\cdot 47 = 141 \\\\equiv 41$\\n- $3^{13} = 3 \\\\cdot 41 = 123 \\\\equiv 23$\\n- $3^{14} = 3 \\\\cdot 23 = 69$\\n- $3^{15} = 3 \\\\cdot 69 = 207 \\\\equiv 7$\\n- $3^{16} = 3 \\\\cdot 7 = 21$\\n- $3^{17} = 3 \\\\cdot 21 = 63$\\n- $3^{18} = 3 \\\\cdot 63 = 189 \\\\equiv 89$\\n- $3^{19} = 3 \\\\cdot 89 = 267 \\\\equiv 67$\\n- $3^{20} = 3 \\\\cdot 67 = 201 \\\\equiv 1 \\\\pmod{100}$ â† Wait! $3^{20} \\\\equiv 1$?\\n\\nWait â€” earlier we said $3^{40} \\\\equiv 1$, but now weâ€™re getting $3^{20} \\\\equiv 1$?\\n\\nLetâ€™s double-check $3^{20}$:\\n\\nWe had:\\n- $3^{10} \\\\equiv 49$\\n- So $3^{20} = (3^{10})^2 = 49^2 = 2401$\\n\\n$$\\n2401 \\\\mod 100 = 1\\n$$\\n\\nYes! So $3^{20} \\\\equiv 1 \\\\pmod{100}$\\n\\nThat means the order is actually 20, not 40. So Euler's theorem gives an exponent (40), but the actual order divides 40, and in this case it's 20.\\n\\nSo since $3^{20} \\\\equiv 1$, then:\\n\\n$$\\n3^{12345} = 3^{25} = 3^{20} \\\\cdot 3^5 \\\\equiv 1 \\\\cdot 3^5 = 243 \\\\equiv 43 \\\\pmod{100}\\n$$\\n\\nWait â€” but earlier we reduced exponent mod 40 and got 25, which is fine, but now we see that the cycle is only 20, so better to reduce exponent mod 20:\\n\\n$$\\n12345 \\\\mod 20 = ?\\n$$\\n\\n$12345 \\\\div 20 = 617.25$, $20 \\\\cdot 617 = 12340$, remainder 5.\\n\\nSo:\\n\\n$$\\n12345 \\\\equiv 5 \\\\pmod{20} \\\\Rightarrow 3^{12345} \\\\equiv 3^5 \\\\pmod{100}\\n$$\\n\\nAnd $3^5 = 243 \\\\Rightarrow 243 \\\\mod 100 = 43$\\n\\nSo final answer is:\\n\\n$$\\n3^{12345} \\\\equiv 43 \\\\pmod{100}\\n$$\\n\\nBut the proposed answer was **36**, which is incorrect.\\n\\n---\\n\\n### âœ… Final Answer:\\n\\n$$\\n\\\\boxed{43}\\n$$\\n\\nThus, the given answer **36** is:\\n\\n**incorrect**\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1385, 'prompt_tokens': 76, 'total_tokens': 1461, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-78237a79-e548-99dd-b096-df3fc0b3ad59', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bbb68-56f2-75d3-ba06-19b9cd3bfc46-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 76, 'output_tokens': 1385, 'total_tokens': 1461, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is 3^{12345} (mod 100)?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54f985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
